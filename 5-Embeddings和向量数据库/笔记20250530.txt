帮我编写Python，读取 员工基本信息表.xlsx 和 员工绩效表.xlsx 的前5行数据
帮我将这两张Excel合并，即在 员工基本信息表.xlsx的基础上，增加该员工 2024年第4季度的绩效评分
合并后的数据保存到 .xlsx

停用词（就是一些虚词，没有太多意义的，不能代表句子特征）：常见的单词，比如 a, the, in，
中文，比如：我、你、和
这些单词的词频很高，但是特征不明显；

Q：多一个酒店，多一个单词，特征个数增加吗
会增加

Q：特征维度，长度不一致，按最长的维度补充吗？
我们需要建立统一的标准，把所有的酒店，都按照这个标准来进行统计

Q：这个推荐系统实现的方法是通用的方法吗，比如和淘宝，抖音的推荐系统的实现方法是否一样
淘宝、抖音的推荐系统 更复杂；是多种推荐算法的结合
你现在看到的咱们的这个项目（酒店推荐系统），只是其中的一种推荐算法；（基于内容的推荐）
淘宝、抖音，还会基于 用户的行为进行推荐

Q：ngram 和 tfidf 都看懂了，他俩咋结合的没懂
酒店的向量描述 = 特征表达 => 用 ngram作为酒店的特征
tfidf = 是单词单词的升级 => 原来只统计 ngram特征出现的次数，现在还考虑了idf => 特征值就会更准确

Q：tf和id f是相乘么？
是的

如果是中文，可以用jieba进行分词。
稀疏矩阵，就是很多特征，对于某个酒店来说，没有存在，所以tf-idf=0，很多值为0

https://huggingface.co/spaces/mteb/leaderboard

原始特征又大又稀 => 压缩到一个固定空间中 （特征降维）=> 特征值就变得稠密 （是原始特征的一种近似表达）

Q：就是神经网络训练之后把特征提炼浓缩了呗
是的，因为input的维度很大，比如 10000维
但是压缩到了隐藏层中，这个维度 << input维度
隐藏层的维度，是人工设定的，比如你可以设定 50维，或者 300维

Word2Vec是一个embedding训练的工具，是采用神经网络的方法
神经网络包括了 前向传播 + 反向传播 => 通过不断的迭代（学习）=> 目的是找到更好的参数，可以得到output更好的结果

output 最终理想的结果，应该是确定的。
因为一旦你把 语料（英文百科全书）给到Word2Vec，它就可以统计，某个单词 邻居的概率。
比如这个单词是apple，邻居的定义是 window=2
因为 英文百科全书是固定的，所以所有 Apple的前后两个邻居，都可以统计出来 => output就可以确定

希望可以用 压缩后的300维，来预测得到output' 和实际的output更接近

min_count 参数是什么意思？

Q：进入空间的第一个词没有任何邻居，是怎么理解语义的？
邻居的定义是基于window，包括前面的window，以及后面的window
如果第一个单词，没有前面的window，但是也会有后面的window

I love eating apple.
对于I来说，如果window=2，我们可以统计到它的邻居 = love 和 eating

I love eating apple.
I love eating banana.
I love eating orange.
XXXX Yes, I can.

Q: 文字的加减得出的结论实际应用中有什么意义？
一般在实际应用中，不怎么做文字加减。主要做 文字相似度的计算
相似度 => 用于做推荐

Q：孙悟空大闹了什么？
如果在《西游记》里写的是：孙行者大闹天宫

Q：向量为什么要加减
一般向量 最常用的业务场景，就是 similarity
我们想说，如果向量学习好，是可以做各种语义理解任务。哪怕是 向量的加减
king-man + woman ~= queen

Q: 它是怎么训练出来，孙悟空和孙行者的相似度这么高呢
孙行者 大闹 黑风山
孙悟空 三岛 求方
孙悟空 大闹 天宫
为什么 apple, banana, orange 如果把50维度的向量可视化出来，会比较接近？
因为他们的 邻居很像啊

Q：具体是怎样从10000维压缩到300维的？

Q：word2vec神经网络 一共有3层，如何学习出来的 300维的embedding？
你可以把 神经网络当成黑盒子，给它<input> <output>（这两个是确定的）
就可以训练出来一个神经网络，来拟合你的  input 和 output的计算逻辑
<input 10000> <embedding 300> <output 10000>


西游记为例，如果我们把《西游记》给到了word2vec，input和output是不是就确定了

如果没有做任何的压缩，我们就是 <input> <output>的全连接
input是1万维，output也是1万维，全连接的矩阵大小就是 [10000, 10000]
保存了的是，比如 孙悟空的邻居是谁？

Q: CRUD和一般的Mysql有什么区别
faiss中，需要设置索引的方式，比如 IndexFlatL2

Q： 老师 对这些具体的知识库，需要对向量数据库做特定的设置码？可能不同知识库的特征不一样，对检索的算法要求不一样
向量数据库，如果设置的话，一般设置的是 索引方式（精确索引，速度快的索引）


Q：向量的数据量 ，和我们实际数据量一样的吧。 比方说 1w个酒店，向量数据库就是 1w个向量。
是的
向量数据库是用于什么？ 就是保存我们的 item的向量特征
item = 酒店
item = 淘宝商品
item = 文档chunk
向量就是这些item的特征表达


Q：老师见过使用mysql当向量数据的么？有多大影响？
mysql是精确查找，如果用于相似度检索，会有些麻烦
select ... from ...
数据库是帮我们存储数据 + 检索数据的。
MySQL是按照 精确查找的方式来进行存储和检索的；

AI大模型的应用，对于数据使用，是有两方面需求的：
1）向量数据库 Faiss => 做的是 RAG检索（相似文档chunk的检索） 
2）MySQL数据 => Text2SQL，让大模型帮你写SQL语句，然后在MYSQL中进行精确的查找


我们的文本会很大，放到向量数据库中，是embedding的状态（嵌入的状态）

Q：Chunk是干嘛的
chunk是一个片段。比如一个文档有10000字，如果以这个文档为颗粒度，放到faiss中 => 就太粗了
可以把这个文档，分成10个chunk，每个chunk不超过1000字，放到faiss中 => 就会有10个chunk（片段）

如果做相似度检索，就会在chunk（片段）的粒度上进行召回

向量数据库中，原始内容就是个备注，主要存储和计算的是向量（向量 = 原始内容的近似理解，压缩表达）
word2vec的理解和使用
内容推荐系统的原理





